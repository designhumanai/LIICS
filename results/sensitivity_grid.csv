H_bits_per_token,V_millions_tokens,Mean_Psi_e11,Std_Psi_e11,CV_percent,CI_lower_e11,CI_upper_e11,Domain_Interpretation,Corpus_Size_Example
1.8,0.7,0.80,0.13,16.5,0.54,1.06,Easy text - low perplexity,Small validation (WikiText-103)
1.8,1.0,1.14,0.19,16.5,0.76,1.52,Easy text - standard corpus,Standard validation (10^6 tokens)
1.8,1.3,1.48,0.24,16.5,1.00,1.96,Easy text - large corpus,Extended validation
2.0,0.7,0.89,0.15,16.5,0.59,1.19,General text - small corpus,Conservative normalization
2.0,1.0,1.27,0.21,16.5,0.85,1.69,General text - CANONICAL,CANONICAL LIICS VALUE
2.0,1.3,1.65,0.27,16.5,1.11,2.19,General text - large corpus,Extended general domain
2.2,0.7,0.98,0.16,16.5,0.66,1.30,Complex text - small corpus,High-entropy domain (code/math)
2.2,1.0,1.40,0.23,16.5,0.94,1.86,Complex text - standard,Code/math domain normalization
2.2,1.3,1.81,0.30,16.5,1.21,2.41,Complex text - large corpus,Specialized high-entropy corpora

# SENSITIVITY ANALYSIS METADATA
# ==============================
# Purpose: Quantify robustness of Psi_LLM to normalization parameter uncertainties
# Method: Calculate mean Psi over 3 compute-optimal models (Chinchilla, PaLM, LLaMA-65B)
#         for systematic grid of (H, V) values
# Key Finding: Coefficient of variation (CV) remains constant at 16.5% across parameter space
#              This is a mathematical consequence of linear scaling Psi ∝ H×V
#              Relative model rankings are preserved under normalization changes

# PARAMETER RANGES
# ================
# Domain Entropy (H): 1.8 - 2.2 bits/token
#   - Lower bound (1.8): Easy text with low perplexity (PPL ≈ 3.5)
#   - Canonical (2.0): General text, typical LLM benchmarks (PPL ≈ 4.0)
#   - Upper bound (2.2): Complex domains (code, mathematics, PPL ≈ 4.6)
#
# Validation Set Size (V): 0.7 - 1.3 million tokens
#   - Lower bound (0.7M): Minimal stable evaluation corpus
#   - Canonical (1.0M): Standard validation set size
#   - Upper bound (1.3M): Extended corpus for domain-specific tasks

# COLUMN DEFINITIONS
# ==================
# H_bits_per_token: Domain entropy (bits/token), H = log2(Perplexity)
# V_millions_tokens: Validation set size (millions of tokens)
# Mean_Psi_e11: Mean Psi_LLM × 10^11 over compute-optimal models
# Std_Psi_e11: Standard deviation × 10^11
# CV_percent: Coefficient of variation (Std/Mean × 100%)
# CI_lower_e11: 95% confidence interval lower bound × 10^11
# CI_upper_e11: 95% confidence interval upper bound × 10^11
# Domain_Interpretation: Practical domain mapping
# Corpus_Size_Example: Example evaluation corpora

# DETAILED UNCERTAINTY PROPAGATION
# =================================
# Combined relative uncertainty (first-order error analysis):
#
#   δPsi/Psi = sqrt[(δH/H)² + (δV/V)² + (δN/N)² + (δD/D)² + (δL/L)² + (δE/E)²]
#
# Typical uncertainties:
#   - δH/H ≈ 10% (entropy measurement variability across domains)
#   - δV/V ≈ 30% (validation set size choice - DOMINANT SOURCE)
#   - δN/N ≈ 5% (parameter count precision from papers)
#   - δD/D ≈ 10% (tokenization variations, checkpoint selection)
#   - δL/L ≈ 0% (architectural specification, exact)
#   - δE/E ≈ 0% (architectural specification, exact)
#
# Total propagated uncertainty:
#   δPsi/Psi ≈ sqrt(0.10² + 0.30² + 0.05² + 0.10²) ≈ 0.326 ≈ 32.6%
#
# This matches observed 95% CI margin of error (±40.9% relative)

# CONSTANT CV INTERPRETATION
# ===========================
# Mathematical consequence, not physical insight:
#   - Psi_LLM = k × L × E, where k ∝ H × V
#   - Therefore: Psi_model1 / Psi_model2 = (k1×L1×E1) / (k2×L2×E2)
#   - Since k ∝ H×V for all models: relative ratios unchanged
#   - CV = Std/Mean is scale-invariant under linear transformations
#
# Physical insight:
#   - Relative model performance rankings are robust to normalization choice
#   - Absolute Psi values depend on (H, V), but architectural comparisons do not
#   - This validates using Psi for cross-model efficiency comparisons

# DOMAIN-SPECIFIC CALIBRATION RECOMMENDATIONS
# ============================================
# General Text (H ≈ 2.0, V ≈ 1M):
#   - Use canonical Psi_LLM = 1.27 × 10^-11
#   - Applicable to: WikiText, C4, OpenWebText, Common Crawl
#
# Code Domain (H ≈ 2.5, V ≈ 1M):
#   - Recalibrate: Psi_code ≈ 1.59 × 10^-11 (25% higher)
#   - Measure H directly from code validation perplexity
#   - Applicable to: GitHub, Stack Overflow, documentation
#
# Mathematics (H ≈ 3.0, V ≈ 1M):
#   - Recalibrate: Psi_math ≈ 1.91 × 10^-11 (50% higher)
#   - High entropy reflects formal notation and proof structure
#   - Applicable to: MATH, GSM8K, theorem proving
#
# Multimodal (H ≈ ?, V ≈ ?):
#   - Open research question
#   - Requires defining token-level entropy for vision/audio
#   - May need architecture-specific Psi for cross-modal attention

# SENSITIVITY TO ARCHITECTURAL PARAMETERS
# ========================================
# Current analysis assumes L, E, N, D are exact (δL/L = δE/E = 0%)
# In practice:
#   - Layer count (L): Exact from architecture specification
#   - Embedding dim (E): Exact from architecture specification
#   - Parameter count (N): ±5% uncertainty from:
#       * Tied embeddings (input/output)
#       * Layer normalization details
#       * Bias terms inclusion
#   - Training data (D): ±10% uncertainty from:
#       * Tokenization method (BPE, SentencePiece, WordPiece)
#       * Deduplication strategies
#       * Checkpoint selection (not always final convergence)
#
# These uncertainties are smaller than H and V, but non-negligible

# COMPARISON WITH ALTERNATIVE INVARIANT FORMS
# ============================================
# Psi_1 (used): Psi = (L×E×H×V)/(N×D)
#   - CV = 16.5% (compute-optimal models)
#   - Physical motivation: Processing depth normalization
#
# Psi_2 (rejected): Psi = (L×E²×H×V)/(N×D)
#   - CV = 20.2% (higher variance)
#   - Motivation: FLOP-based normalization (E² from attention)
#   - Rejected: No improvement in stability vs. added complexity
#
# Psi_3 (rejected): Psi = (L×E×H×V)/(N×D^0.5)
#   - CV = 27.1% (highest variance)
#   - Motivation: Fractional data scaling
#   - Rejected: Lacks theoretical justification, worse empirical fit

# IMPLICATIONS FOR RESOURCE PLANNING
# ===================================
# Conservative approach (H = 2.2, V = 1.3M):
#   - Use Psi_LLM = 1.81 × 10^-11 (upper bound)
#   - Predicts LOWER D_max → saves computational resources
#   - Risk: May undertrain if actual domain has H < 2.2
#
# Aggressive approach (H = 1.8, V = 0.7M):
#   - Use Psi_LLM = 0.80 × 10^-11 (lower bound)
#   - Predicts HIGHER D_max → requires more data
#   - Risk: Wastes resources if domain has H > 1.8
#
# Recommended approach:
#   1. Measure validation perplexity on target domain
#   2. Calculate H = log2(PPL_val) directly
#   3. Choose V = 10^6 as baseline (adjust for stability)
#   4. Use ±32% uncertainty bounds for planning buffers

# FUTURE VALIDATION PRIORITIES
# =============================
# 1. Direct entropy measurement: Calculate H from actual validation losses
# 2. Corpus size optimization: Determine minimal V for stable Psi estimation
# 3. Domain-specific invariants: Test Psi stability on code/math benchmarks
# 4. Architectural extensions: Validate on MoE, SSM, RWKV architectures
# 5. Dynamic tracking: Measure G_S(C)(t) evolution during training

# DATA SOURCES
# ============
# Base models: results/psi_values.csv (Chinchilla, PaLM, LLaMA-65B)
# Calculation: scripts/sensitivity_analysis.py
# Statistical method: Grid search over (H, V) with fixed model parameters

# VERSION HISTORY
# ===============
# v2.1 (2025-11-14): Extended grid to 9 parameter combinations
#                    Added domain calibration recommendations
# v2.0 (2025-11-13): Initial sensitivity analysis (5 combinations)
# v1.0 (2025-11-12): Pilot uncertainty quantification

# REPRODUCIBILITY
# ===============
# Python code: scripts/sensitivity_analysis.py --grid
# Visualization: scripts/sensitivity_analysis.py --plot
# Tests: tests/test_calculations.py::TestSensitivity
# Repository: https://github.com/designhumanai/liics
